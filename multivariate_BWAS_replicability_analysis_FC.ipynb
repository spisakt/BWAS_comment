{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Analyzing replicability of functional connectivity-based multivariate BWAS on the Human Connectome Project dataset\n",
    "\n",
    "This notebook contains the main analyses of the Matters Arising, with 6 example phenotypes.\n",
    "For a comprehensive analysis of 52 HCP phenotypes, see `multivariate_BWAS_replicability_analysis_FC_extensive.ipynb`.\n",
    "For an analysis with finer sample size resolution, see `multivariate_BWAS_replicability_analysis_FC_hires.ipynb`.\n",
    "For an analysis with cortical thickness data, see `multivariate_BWAS_replicability_analysis_CT.ipynb`.\n",
    "\n",
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-03T20:04:15.431840Z",
     "start_time": "2021-08-03T20:04:14.753565Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.model_selection import KFold, train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.decomposition import PCA\n",
    "from joblib import Parallel, delayed\n",
    "from mlxtend.evaluate import permutation_test\n",
    "sns.set(rc={\"figure.figsize\":(4, 2)})\n",
    "sns.set_style(\"whitegrid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load HCP data\n",
    "\n",
    "We load functional network matrices (netmats) from the HCP1200-release, as published on connectomeDB: https://db.humanconnectome.org/\n",
    "Due to licensing issues, data is not supplied with the repository, but can be downloaded from the ConnectomeDB or via `get_data.ipynb` (requires credentials).\n",
    "See [readme.md](readme.md) for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "          T1_Count     T2_Count  3T_RS-fMRI_Count  3T_RS-fMRI_PctCompl  \\\ncount  1206.000000  1206.000000       1206.000000          1206.000000   \nmean      1.478441     1.400498          3.509950            87.213267   \nstd       0.635688     0.628216          1.215181            31.027886   \nmin       0.000000     0.000000          0.000000             0.000000   \n25%       1.000000     1.000000          4.000000           100.000000   \n50%       2.000000     1.000000          4.000000           100.000000   \n75%       2.000000     2.000000          4.000000           100.000000   \nmax       2.000000     2.000000          4.000000           100.000000   \n\n       3T_tMRI_PctCompl  fMRI_WM_PctCompl  fMRI_Gamb_PctCompl  \\\ncount       1206.000000       1206.000000         1206.000000   \nmean          88.219569         89.718076           89.852736   \nstd           29.942161         30.384864           30.163559   \nmin            0.000000          0.000000            0.000000   \n25%          100.000000        100.000000          100.000000   \n50%          100.000000        100.000000          100.000000   \n75%          100.000000        100.000000          100.000000   \nmax          100.000000        100.000000          100.000000   \n\n       fMRI_Mot_PctCompl  fMRI_Lang_PctCompl  fMRI_Soc_PctCompl  ...  \\\ncount        1206.000000         1206.000000        1206.000000  ...   \nmean           89.631675           87.027114          87.039801  ...   \nstd            30.492092           33.566043          33.570248  ...   \nmin             0.000000            0.000000           0.000000  ...   \n25%           100.000000          100.000000         100.000000  ...   \n50%           100.000000          100.000000         100.000000  ...   \n75%           100.000000          100.000000         100.000000  ...   \nmax           100.000000          100.000000         100.000000  ...   \n\n        Odor_Unadj  Odor_AgeAdj  PainIntens_RawScore  PainInterf_Tscore  \\\ncount  1204.000000  1204.000000          1201.000000        1205.000000   \nmean    110.421321    97.727500             1.449625          45.847718   \nstd       9.107963    11.273251             1.783069           7.679288   \nmin      82.740000    59.860000             0.000000          38.600000   \n25%     101.120000    87.110000             0.000000          38.600000   \n50%     108.790000    98.040000             1.000000          45.900000   \n75%     122.250000   110.450000             2.000000          52.200000   \nmax     122.250000   111.410000            10.000000          75.300000   \n\n       Taste_Unadj  Taste_AgeAdj  Mars_Log_Score    Mars_Errs   Mars_Final  \\\ncount  1200.000000   1200.000000     1198.000000  1195.000000  1195.000000   \nmean     95.166983     93.998533        1.845467     0.581590     1.822251   \nstd      14.583412     14.837851        0.541393     0.973172     0.542893   \nmin      56.350000     59.500000        1.560000     0.000000     1.080000   \n25%      84.070000     83.220000        1.800000     0.000000     1.760000   \n50%      95.360000     94.970000        1.800000     0.000000     1.800000   \n75%     105.570000    102.920000        1.880000     1.000000     1.840000   \nmax     134.650000    131.380000       15.000000    17.000000    15.000000   \n\n               age  \ncount  1206.000000  \nmean     28.904229  \nstd       3.570475  \nmin      23.500000  \n25%      28.000000  \n50%      28.000000  \n75%      33.000000  \nmax      36.000000  \n\n[8 rows x 456 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>T1_Count</th>\n      <th>T2_Count</th>\n      <th>3T_RS-fMRI_Count</th>\n      <th>3T_RS-fMRI_PctCompl</th>\n      <th>3T_tMRI_PctCompl</th>\n      <th>fMRI_WM_PctCompl</th>\n      <th>fMRI_Gamb_PctCompl</th>\n      <th>fMRI_Mot_PctCompl</th>\n      <th>fMRI_Lang_PctCompl</th>\n      <th>fMRI_Soc_PctCompl</th>\n      <th>...</th>\n      <th>Odor_Unadj</th>\n      <th>Odor_AgeAdj</th>\n      <th>PainIntens_RawScore</th>\n      <th>PainInterf_Tscore</th>\n      <th>Taste_Unadj</th>\n      <th>Taste_AgeAdj</th>\n      <th>Mars_Log_Score</th>\n      <th>Mars_Errs</th>\n      <th>Mars_Final</th>\n      <th>age</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>1206.000000</td>\n      <td>1206.000000</td>\n      <td>1206.000000</td>\n      <td>1206.000000</td>\n      <td>1206.000000</td>\n      <td>1206.000000</td>\n      <td>1206.000000</td>\n      <td>1206.000000</td>\n      <td>1206.000000</td>\n      <td>1206.000000</td>\n      <td>...</td>\n      <td>1204.000000</td>\n      <td>1204.000000</td>\n      <td>1201.000000</td>\n      <td>1205.000000</td>\n      <td>1200.000000</td>\n      <td>1200.000000</td>\n      <td>1198.000000</td>\n      <td>1195.000000</td>\n      <td>1195.000000</td>\n      <td>1206.000000</td>\n    </tr>\n    <tr>\n      <th>mean</th>\n      <td>1.478441</td>\n      <td>1.400498</td>\n      <td>3.509950</td>\n      <td>87.213267</td>\n      <td>88.219569</td>\n      <td>89.718076</td>\n      <td>89.852736</td>\n      <td>89.631675</td>\n      <td>87.027114</td>\n      <td>87.039801</td>\n      <td>...</td>\n      <td>110.421321</td>\n      <td>97.727500</td>\n      <td>1.449625</td>\n      <td>45.847718</td>\n      <td>95.166983</td>\n      <td>93.998533</td>\n      <td>1.845467</td>\n      <td>0.581590</td>\n      <td>1.822251</td>\n      <td>28.904229</td>\n    </tr>\n    <tr>\n      <th>std</th>\n      <td>0.635688</td>\n      <td>0.628216</td>\n      <td>1.215181</td>\n      <td>31.027886</td>\n      <td>29.942161</td>\n      <td>30.384864</td>\n      <td>30.163559</td>\n      <td>30.492092</td>\n      <td>33.566043</td>\n      <td>33.570248</td>\n      <td>...</td>\n      <td>9.107963</td>\n      <td>11.273251</td>\n      <td>1.783069</td>\n      <td>7.679288</td>\n      <td>14.583412</td>\n      <td>14.837851</td>\n      <td>0.541393</td>\n      <td>0.973172</td>\n      <td>0.542893</td>\n      <td>3.570475</td>\n    </tr>\n    <tr>\n      <th>min</th>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>...</td>\n      <td>82.740000</td>\n      <td>59.860000</td>\n      <td>0.000000</td>\n      <td>38.600000</td>\n      <td>56.350000</td>\n      <td>59.500000</td>\n      <td>1.560000</td>\n      <td>0.000000</td>\n      <td>1.080000</td>\n      <td>23.500000</td>\n    </tr>\n    <tr>\n      <th>25%</th>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>4.000000</td>\n      <td>100.000000</td>\n      <td>100.000000</td>\n      <td>100.000000</td>\n      <td>100.000000</td>\n      <td>100.000000</td>\n      <td>100.000000</td>\n      <td>100.000000</td>\n      <td>...</td>\n      <td>101.120000</td>\n      <td>87.110000</td>\n      <td>0.000000</td>\n      <td>38.600000</td>\n      <td>84.070000</td>\n      <td>83.220000</td>\n      <td>1.800000</td>\n      <td>0.000000</td>\n      <td>1.760000</td>\n      <td>28.000000</td>\n    </tr>\n    <tr>\n      <th>50%</th>\n      <td>2.000000</td>\n      <td>1.000000</td>\n      <td>4.000000</td>\n      <td>100.000000</td>\n      <td>100.000000</td>\n      <td>100.000000</td>\n      <td>100.000000</td>\n      <td>100.000000</td>\n      <td>100.000000</td>\n      <td>100.000000</td>\n      <td>...</td>\n      <td>108.790000</td>\n      <td>98.040000</td>\n      <td>1.000000</td>\n      <td>45.900000</td>\n      <td>95.360000</td>\n      <td>94.970000</td>\n      <td>1.800000</td>\n      <td>0.000000</td>\n      <td>1.800000</td>\n      <td>28.000000</td>\n    </tr>\n    <tr>\n      <th>75%</th>\n      <td>2.000000</td>\n      <td>2.000000</td>\n      <td>4.000000</td>\n      <td>100.000000</td>\n      <td>100.000000</td>\n      <td>100.000000</td>\n      <td>100.000000</td>\n      <td>100.000000</td>\n      <td>100.000000</td>\n      <td>100.000000</td>\n      <td>...</td>\n      <td>122.250000</td>\n      <td>110.450000</td>\n      <td>2.000000</td>\n      <td>52.200000</td>\n      <td>105.570000</td>\n      <td>102.920000</td>\n      <td>1.880000</td>\n      <td>1.000000</td>\n      <td>1.840000</td>\n      <td>33.000000</td>\n    </tr>\n    <tr>\n      <th>max</th>\n      <td>2.000000</td>\n      <td>2.000000</td>\n      <td>4.000000</td>\n      <td>100.000000</td>\n      <td>100.000000</td>\n      <td>100.000000</td>\n      <td>100.000000</td>\n      <td>100.000000</td>\n      <td>100.000000</td>\n      <td>100.000000</td>\n      <td>...</td>\n      <td>122.250000</td>\n      <td>111.410000</td>\n      <td>10.000000</td>\n      <td>75.300000</td>\n      <td>134.650000</td>\n      <td>131.380000</td>\n      <td>15.000000</td>\n      <td>17.000000</td>\n      <td>15.000000</td>\n      <td>36.000000</td>\n    </tr>\n  </tbody>\n</table>\n<p>8 rows Ã— 456 columns</p>\n</div>"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# HCP data can be obtained from the connectomeDB\n",
    "# data is not part of this repository\n",
    "subjectIDs = pd.read_csv('hcp_data/subjectIDs.txt', header=None)\n",
    "\n",
    "netmats_pearson = pd.read_csv('hcp_data/netmats1_correlationZ.txt',\n",
    "                             sep=' ',\n",
    "                             header=None)\n",
    "netmats_pearson['ID'] = subjectIDs[0]\n",
    "netmats_pearson.set_index('ID', drop=True, inplace=True)\n",
    "\n",
    "\n",
    "netmats_parcor = pd.read_csv('hcp_data/netmats2_partial-correlation.txt',\n",
    "                             sep=' ',\n",
    "                             header=None)\n",
    "netmats_parcor['ID'] = subjectIDs[0]\n",
    "netmats_parcor.set_index('ID', drop=True, inplace=True)\n",
    "\n",
    "behavior = pd.read_csv('hcp_data/hcp1200_behavioral_data.csv')\n",
    "behavior = behavior.set_index('Subject', drop=True)\n",
    "\n",
    "# convert age to numeric\n",
    "age = []\n",
    "for s in behavior['Age']:\n",
    "    if s == '36+':\n",
    "        age.append(36)\n",
    "    else:\n",
    "        split = s.split(sep='-')\n",
    "        age.append(np.mean((float(split[0]), float(split[1]))))\n",
    "\n",
    "behavior['age'] = age\n",
    "behavior.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper function to prepare target variable\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_data(target='CogTotalComp_AgeAdj', feature_data=netmats_parcor):\n",
    "    # it's a good practice to use pandas for merging, messing up subject order can be painful\n",
    "    features = feature_data.columns\n",
    "    df = behavior\n",
    "    df = df.merge(feature_data, left_index=True, right_index=True, how='left')\n",
    "\n",
    "    df = df.dropna(subset = [target] + features.values.tolist())\n",
    "    y = df[target].values\n",
    "    X = df[features].values\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper function implementing a single bootstrap iteration\n",
    "\n",
    "We define a workhorse function which:\n",
    "- randomly samples the discovery and the replication datasets,\n",
    "- creates cross-validated estimates of predictive performance within the discovery sample\n",
    "- finalizes the model by fitting it to the whole discovery sample (overfits the discovery but not the replication sample)\n",
    "- use it to predict the replication sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def corr(X, Y):\n",
    "    # just a small wrapper function (pandas correlation is silent in \"unlucky\" bootstraps with constant values)\n",
    "    return pd.Series(X).corr( pd.Series(Y))\n",
    "\n",
    "def bootstrap_workhorse(X, y, sample_size, model, random_state, shuffle_y=False):\n",
    "\n",
    "    #create discovery and replication samples by random sampling from the whole dataset (without replacement)\n",
    "\n",
    "    # if shuffle_y is true, a null model is created bz permuting y\n",
    "    if shuffle_y:\n",
    "        rng = np.random.default_rng(random_state)\n",
    "        y = rng.permutation(y)\n",
    "\n",
    "    # sample the discovery and replication sets *without replacement* (with replacement introduces spurious dependencies)\n",
    "    X_discovery, X_replication, y_discovery, y_replication = train_test_split(X, y, train_size=sample_size, test_size=sample_size, shuffle=True, random_state=random_state)\n",
    "\n",
    "    # standard 10-fold cross-validation\n",
    "    cv = KFold(10)\n",
    "\n",
    "    # below we obtain cross-validated predictions in the discovery sample\n",
    "    predicted_discovery_cv = np.zeros_like(y_discovery)  # here we collect the predictions for each fold\n",
    "    cor_per_fold = np.zeros(cv.n_splits)  # here we collect the predictive performance in each fold\n",
    "    i = 0  # just a counter\n",
    "    for train, test in cv.split(X=X_discovery, y=y_discovery):  # loop to leave one fold out\n",
    "        model.fit(X=X_discovery[train], y=y_discovery[train]) # fit model to the training set\n",
    "        predicted_discovery_cv[test] = model.predict(X=X_discovery[test]) # use fitted model to predict teh test set\n",
    "        cor_per_fold[i] = corr(y_discovery[test], predicted_discovery_cv[test]) # calculate performance on tne test set\n",
    "        i += 1\n",
    "    # calculate mean test performance across all folds\n",
    "    r_disc_cv = np.mean(cor_per_fold)\n",
    "    # 'finalize' model by training it on the full discovery sample (without cross-validation)\n",
    "    final_model = model.fit(X=X_discovery, y=y_discovery)\n",
    "    # obtain predictions with the final model on the discovery sample, note that this model actually overfits this sample.\n",
    "    # we do this only to demonstrate biased estimates\n",
    "    predicted_discovery_overfit = final_model.predict(X=X_discovery)\n",
    "    # here we obtain the biased effect size (r) estimates for demonstrational purposes\n",
    "    r_disc_overfit = corr(predicted_discovery_overfit, y_discovery)\n",
    "\n",
    "    # We use the final model to predict the replication sample\n",
    "    # This is correct (no overfitting here), the final model did not see this data during training\n",
    "    predicted_replication = final_model.predict(X=X_replication)\n",
    "    # we obtain the out-of-sample prediction performance estimates\n",
    "    r_rep = corr(predicted_replication, y_replication)\n",
    "\n",
    "    # below we calculate permutation-based p-values for all three effect size estimates (in-sample unbiased, in-sample biased, out-of-sample)\n",
    "    # (one sided tests, testing for positive correlation)\n",
    "    p_disc_cv = permutation_test(predicted_discovery_cv, y_discovery, method='approximate', num_rounds=1000, func=lambda x, y: corr(x, y),seed=random_state)\n",
    "    p_disc_overfit = permutation_test(predicted_discovery_overfit, y_discovery, method='approximate', num_rounds=1000, func=lambda x, y: corr(x, y),seed=random_state)\n",
    "    p_rep = permutation_test(predicted_replication, y_replication, method='approximate', num_rounds=1000, func=lambda x, y: corr(x, y),seed=random_state)\n",
    "    # return results\n",
    "    return r_disc_cv, r_disc_overfit, r_rep, p_disc_cv, p_disc_overfit, p_rep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All set, now we start the analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Replicability with sample sizes n=50, 100, 200, 300 and max\n",
    "Here we train a few different models on 100 bootstrap samples.\n",
    "\n",
    "We aggregate the results of our workhorse function in `n_bootstrap`=100 bootstrap cases (run in parallel).\n",
    "\n",
    "The whole process is repeated for all sample sizes, fetaure_sets and target variables.\n",
    "\n",
    "## Here we test age and 5 cognitive variables, including 'cognitive ability' (the main target variable in the target paper)\n",
    "- age: age group of the participants\n",
    "- CogTotalComp_AgeAdj: total cognitive ability\n",
    "- PMAT24_A_CR, : Fluid Intelligence (Penn Progressive Matrices)\n",
    "- CardSort_AgeAdj: Executive Function/Cognitive Flexibility (Dimensional Change Card Sort)\n",
    "- Flanker_AgeAdj: Executive Function/Inhibition (Flanker Task)\n",
    "- PicSeq_AgeAdj: Episodic Memory (Picture Sequence Memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reproducing the PCA+SVR-based model from the target paper\n",
    "- Both PCA and SVR are done inside the cross-validation\n",
    "- PCA reatains the firts k principal components that together explain 50% of the variance\n",
    "- scikit-learn makes sure that PCA is only fit for the training samples\n",
    "- both for the test sets (in the cross-validation) and the replication sample PCA is not re-fit, bt features are simply transformed with the already fit PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 46.5 s, sys: 6.18 s, total: 52.7 s\n",
      "Wall time: 3h 21min 54s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%%capture\n",
    "\n",
    "random_state = 42\n",
    "n_bootstrap = 100\n",
    "\n",
    "features = {\n",
    "    'netmats_parcor': netmats_parcor,\n",
    "    'netmats_pearson': netmats_pearson\n",
    "}\n",
    "\n",
    "models = {\n",
    "    'PCA_SVR': Pipeline([('pca', PCA(n_components=0.5)),\n",
    "                         ('svr', SVR())])\n",
    "\n",
    "}\n",
    "\n",
    "# We aggregate all results here:\n",
    "df = pd.DataFrame(columns=['connectivity','model','target','n','r_discovery_cv','r_discovery_overfit','r_replication','p_discovery_cv','p_discovery_overfit','p_replication'])\n",
    "\n",
    "for feature_set in features:\n",
    "    for model in models:\n",
    "        for target_var in ['age', 'CogTotalComp_AgeAdj', 'PMAT24_A_CR', 'Flanker_AgeAdj', 'CardSort_AgeAdj', 'PicSeq_AgeAdj']:\n",
    "            for sample_size in [50, 100, 200, 300, 'max']:\n",
    "\n",
    "                print('*****************************************************************')\n",
    "                print(feature_set, model, target_var, sample_size)\n",
    "\n",
    "                X, y = create_data(target=target_var, feature_data=features[feature_set]);\n",
    "\n",
    "                if sample_size=='max':\n",
    "                    sample_size = int(len(y)/2)\n",
    "\n",
    "                # create random seeds for each bootstrap iteration for reproducibility\n",
    "                rng = np.random.default_rng(random_state)\n",
    "                random_sates = rng.integers(np.iinfo(np.int32).max, size=n_bootstrap)\n",
    "\n",
    "                # run bootstrap iterations in parallel\n",
    "                r_discovery_cv, r_discovery_overfit, r_replication, p_discovery_cv, p_discovery_overfit, p_replication = zip(\n",
    "                    *Parallel(n_jobs=-1)(\n",
    "                    delayed(bootstrap_workhorse)(X, y, sample_size, models[model], seed) for seed in random_sates))\n",
    "\n",
    "                tmp_data_frame = pd.DataFrame({\n",
    "                    'connectivity' : feature_set,\n",
    "                    'model' : model,\n",
    "                    'target' : target_var,\n",
    "                    'n' : sample_size,\n",
    "                    'r_discovery_cv': r_discovery_cv,\n",
    "                    'r_discovery_overfit': r_discovery_overfit,\n",
    "                    'r_replication': r_replication,\n",
    "                    'p_discovery_cv': p_discovery_cv,\n",
    "                    'p_discovery_overfit': p_discovery_overfit,\n",
    "                    'p_replication': p_replication\n",
    "                })\n",
    "                #sns.scatterplot(x='r_replication', y='r_discovery_cv', data=tmp_data_frame)\n",
    "                #plt.ylabel('in-sample (r)')\n",
    "                #plt.xlabel('out-of-sample (r_pred)')\n",
    "                #plt.show()\n",
    "                print('r discovery (with cv) :', tmp_data_frame.r_discovery_cv.mean(), 'r replication:', tmp_data_frame.r_replication.mean())\n",
    "\n",
    "                for alpha in [0.05, 0.01, 0.005, 0.001]:\n",
    "                    print('Replicability at alpha =', alpha, ':',\n",
    "                          (tmp_data_frame.loc[tmp_data_frame['p_discovery_cv']<alpha,'p_replication']<alpha).sum() / (tmp_data_frame['p_discovery_cv']<0.05).sum() * 100, '%')\n",
    "\n",
    "                df = pd.concat((df, tmp_data_frame))\n",
    "                df.reset_index(drop=True, inplace=True)\n",
    "                df.to_csv('res/results_PCA_SVR.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "data": {
      "text/plain": "                                                 r_discovery_cv  r_replication\nconnectivity    model   target              n                                 \nnetmats_parcor  PCA_SVR CardSort_AgeAdj     50         0.021165       0.060743\n                                            100        0.077595       0.082970\n                                            200        0.097733       0.102561\n                                            300        0.122905       0.123971\n                                            500        0.139694       0.149550\n                        CogTotalComp_AgeAdj 50         0.128129       0.140322\n                                            100        0.190058       0.212629\n                                            200        0.262374       0.271359\n                                            300        0.303186       0.310136\n                                            495        0.337404       0.346112\n                        Flanker_AgeAdj      50         0.015871       0.028663\n                                            100        0.038605       0.044466\n                                            200        0.047259       0.058335\n                                            300        0.073128       0.076713\n                                            501        0.113786       0.122506\n                        PMAT24_A_CR         50         0.097498       0.121062\n                                            100        0.181649       0.166610\n                                            200        0.224645       0.243301\n                                            300        0.265809       0.283676\n                                            499        0.312464       0.325274\n                        PicSeq_AgeAdj       50         0.062696       0.075164\n                                            100        0.063006       0.098759\n                                            200        0.107779       0.124055\n                                            300        0.142772       0.152545\n                                            501        0.180642       0.185303\n                        age                 50         0.184512       0.189014\n                                            100        0.247298       0.273474\n                                            200        0.342017       0.361751\n                                            300        0.387432       0.394381\n                                            501        0.426137       0.443717\nnetmats_pearson PCA_SVR CardSort_AgeAdj     50         0.013299       0.052865\n                                            100        0.066240       0.055112\n                                            200        0.075358       0.073260\n                                            300        0.083765       0.090858\n                                            500        0.105097       0.106389\n                        CogTotalComp_AgeAdj 50         0.026131       0.072575\n                                            100        0.042545       0.063498\n                                            200        0.080139       0.106129\n                                            300        0.123717       0.133728\n                                            495        0.165746       0.175958\n                        Flanker_AgeAdj      50         0.004188       0.026447\n                                            100        0.017319       0.037797\n                                            200        0.040002       0.047852\n                                            300        0.066405       0.073938\n                                            501        0.094710       0.102038\n                        PMAT24_A_CR         50         0.011407       0.039374\n                                            100        0.065420       0.052835\n                                            200        0.083985       0.099913\n                                            300        0.114977       0.141534\n                                            499        0.169373       0.179310\n                        PicSeq_AgeAdj       50         0.030554       0.034935\n                                            100        0.047759       0.061475\n                                            200        0.054747       0.067498\n                                            300        0.069910       0.074586\n                                            501        0.092276       0.100775\n                        age                 50         0.003479       0.055075\n                                            100        0.065102       0.079922\n                                            200        0.111430       0.139865\n                                            300        0.156014       0.158109\n                                            501        0.182829       0.190848",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th>r_discovery_cv</th>\n      <th>r_replication</th>\n    </tr>\n    <tr>\n      <th>connectivity</th>\n      <th>model</th>\n      <th>target</th>\n      <th>n</th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th rowspan=\"30\" valign=\"top\">netmats_parcor</th>\n      <th rowspan=\"30\" valign=\"top\">PCA_SVR</th>\n      <th rowspan=\"5\" valign=\"top\">CardSort_AgeAdj</th>\n      <th>50</th>\n      <td>0.021165</td>\n      <td>0.060743</td>\n    </tr>\n    <tr>\n      <th>100</th>\n      <td>0.077595</td>\n      <td>0.082970</td>\n    </tr>\n    <tr>\n      <th>200</th>\n      <td>0.097733</td>\n      <td>0.102561</td>\n    </tr>\n    <tr>\n      <th>300</th>\n      <td>0.122905</td>\n      <td>0.123971</td>\n    </tr>\n    <tr>\n      <th>500</th>\n      <td>0.139694</td>\n      <td>0.149550</td>\n    </tr>\n    <tr>\n      <th rowspan=\"5\" valign=\"top\">CogTotalComp_AgeAdj</th>\n      <th>50</th>\n      <td>0.128129</td>\n      <td>0.140322</td>\n    </tr>\n    <tr>\n      <th>100</th>\n      <td>0.190058</td>\n      <td>0.212629</td>\n    </tr>\n    <tr>\n      <th>200</th>\n      <td>0.262374</td>\n      <td>0.271359</td>\n    </tr>\n    <tr>\n      <th>300</th>\n      <td>0.303186</td>\n      <td>0.310136</td>\n    </tr>\n    <tr>\n      <th>495</th>\n      <td>0.337404</td>\n      <td>0.346112</td>\n    </tr>\n    <tr>\n      <th rowspan=\"5\" valign=\"top\">Flanker_AgeAdj</th>\n      <th>50</th>\n      <td>0.015871</td>\n      <td>0.028663</td>\n    </tr>\n    <tr>\n      <th>100</th>\n      <td>0.038605</td>\n      <td>0.044466</td>\n    </tr>\n    <tr>\n      <th>200</th>\n      <td>0.047259</td>\n      <td>0.058335</td>\n    </tr>\n    <tr>\n      <th>300</th>\n      <td>0.073128</td>\n      <td>0.076713</td>\n    </tr>\n    <tr>\n      <th>501</th>\n      <td>0.113786</td>\n      <td>0.122506</td>\n    </tr>\n    <tr>\n      <th rowspan=\"5\" valign=\"top\">PMAT24_A_CR</th>\n      <th>50</th>\n      <td>0.097498</td>\n      <td>0.121062</td>\n    </tr>\n    <tr>\n      <th>100</th>\n      <td>0.181649</td>\n      <td>0.166610</td>\n    </tr>\n    <tr>\n      <th>200</th>\n      <td>0.224645</td>\n      <td>0.243301</td>\n    </tr>\n    <tr>\n      <th>300</th>\n      <td>0.265809</td>\n      <td>0.283676</td>\n    </tr>\n    <tr>\n      <th>499</th>\n      <td>0.312464</td>\n      <td>0.325274</td>\n    </tr>\n    <tr>\n      <th rowspan=\"5\" valign=\"top\">PicSeq_AgeAdj</th>\n      <th>50</th>\n      <td>0.062696</td>\n      <td>0.075164</td>\n    </tr>\n    <tr>\n      <th>100</th>\n      <td>0.063006</td>\n      <td>0.098759</td>\n    </tr>\n    <tr>\n      <th>200</th>\n      <td>0.107779</td>\n      <td>0.124055</td>\n    </tr>\n    <tr>\n      <th>300</th>\n      <td>0.142772</td>\n      <td>0.152545</td>\n    </tr>\n    <tr>\n      <th>501</th>\n      <td>0.180642</td>\n      <td>0.185303</td>\n    </tr>\n    <tr>\n      <th rowspan=\"5\" valign=\"top\">age</th>\n      <th>50</th>\n      <td>0.184512</td>\n      <td>0.189014</td>\n    </tr>\n    <tr>\n      <th>100</th>\n      <td>0.247298</td>\n      <td>0.273474</td>\n    </tr>\n    <tr>\n      <th>200</th>\n      <td>0.342017</td>\n      <td>0.361751</td>\n    </tr>\n    <tr>\n      <th>300</th>\n      <td>0.387432</td>\n      <td>0.394381</td>\n    </tr>\n    <tr>\n      <th>501</th>\n      <td>0.426137</td>\n      <td>0.443717</td>\n    </tr>\n    <tr>\n      <th rowspan=\"30\" valign=\"top\">netmats_pearson</th>\n      <th rowspan=\"30\" valign=\"top\">PCA_SVR</th>\n      <th rowspan=\"5\" valign=\"top\">CardSort_AgeAdj</th>\n      <th>50</th>\n      <td>0.013299</td>\n      <td>0.052865</td>\n    </tr>\n    <tr>\n      <th>100</th>\n      <td>0.066240</td>\n      <td>0.055112</td>\n    </tr>\n    <tr>\n      <th>200</th>\n      <td>0.075358</td>\n      <td>0.073260</td>\n    </tr>\n    <tr>\n      <th>300</th>\n      <td>0.083765</td>\n      <td>0.090858</td>\n    </tr>\n    <tr>\n      <th>500</th>\n      <td>0.105097</td>\n      <td>0.106389</td>\n    </tr>\n    <tr>\n      <th rowspan=\"5\" valign=\"top\">CogTotalComp_AgeAdj</th>\n      <th>50</th>\n      <td>0.026131</td>\n      <td>0.072575</td>\n    </tr>\n    <tr>\n      <th>100</th>\n      <td>0.042545</td>\n      <td>0.063498</td>\n    </tr>\n    <tr>\n      <th>200</th>\n      <td>0.080139</td>\n      <td>0.106129</td>\n    </tr>\n    <tr>\n      <th>300</th>\n      <td>0.123717</td>\n      <td>0.133728</td>\n    </tr>\n    <tr>\n      <th>495</th>\n      <td>0.165746</td>\n      <td>0.175958</td>\n    </tr>\n    <tr>\n      <th rowspan=\"5\" valign=\"top\">Flanker_AgeAdj</th>\n      <th>50</th>\n      <td>0.004188</td>\n      <td>0.026447</td>\n    </tr>\n    <tr>\n      <th>100</th>\n      <td>0.017319</td>\n      <td>0.037797</td>\n    </tr>\n    <tr>\n      <th>200</th>\n      <td>0.040002</td>\n      <td>0.047852</td>\n    </tr>\n    <tr>\n      <th>300</th>\n      <td>0.066405</td>\n      <td>0.073938</td>\n    </tr>\n    <tr>\n      <th>501</th>\n      <td>0.094710</td>\n      <td>0.102038</td>\n    </tr>\n    <tr>\n      <th rowspan=\"5\" valign=\"top\">PMAT24_A_CR</th>\n      <th>50</th>\n      <td>0.011407</td>\n      <td>0.039374</td>\n    </tr>\n    <tr>\n      <th>100</th>\n      <td>0.065420</td>\n      <td>0.052835</td>\n    </tr>\n    <tr>\n      <th>200</th>\n      <td>0.083985</td>\n      <td>0.099913</td>\n    </tr>\n    <tr>\n      <th>300</th>\n      <td>0.114977</td>\n      <td>0.141534</td>\n    </tr>\n    <tr>\n      <th>499</th>\n      <td>0.169373</td>\n      <td>0.179310</td>\n    </tr>\n    <tr>\n      <th rowspan=\"5\" valign=\"top\">PicSeq_AgeAdj</th>\n      <th>50</th>\n      <td>0.030554</td>\n      <td>0.034935</td>\n    </tr>\n    <tr>\n      <th>100</th>\n      <td>0.047759</td>\n      <td>0.061475</td>\n    </tr>\n    <tr>\n      <th>200</th>\n      <td>0.054747</td>\n      <td>0.067498</td>\n    </tr>\n    <tr>\n      <th>300</th>\n      <td>0.069910</td>\n      <td>0.074586</td>\n    </tr>\n    <tr>\n      <th>501</th>\n      <td>0.092276</td>\n      <td>0.100775</td>\n    </tr>\n    <tr>\n      <th rowspan=\"5\" valign=\"top\">age</th>\n      <th>50</th>\n      <td>0.003479</td>\n      <td>0.055075</td>\n    </tr>\n    <tr>\n      <th>100</th>\n      <td>0.065102</td>\n      <td>0.079922</td>\n    </tr>\n    <tr>\n      <th>200</th>\n      <td>0.111430</td>\n      <td>0.139865</td>\n    </tr>\n    <tr>\n      <th>300</th>\n      <td>0.156014</td>\n      <td>0.158109</td>\n    </tr>\n    <tr>\n      <th>501</th>\n      <td>0.182829</td>\n      <td>0.190848</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby(['connectivity', 'model', 'target', 'n']).mean()[['r_discovery_cv', 'r_replication']]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now we fit a simple Ridge regression\n",
    "(no feature selection, no hyperparameter optimization)\n",
    "Based on some previous studies, this can be expected to perform better than SVR (especially with lower sample sizes). See the paper for details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 40.8 s, sys: 5.54 s, total: 46.3 s\n",
      "Wall time: 27min 10s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%%capture\n",
    "\n",
    "random_state = 42\n",
    "n_bootstrap = 100\n",
    "\n",
    "features = {\n",
    "    'netmats_parcor': netmats_parcor,\n",
    "    'netmats_pearson': netmats_pearson\n",
    "}\n",
    "\n",
    "models = {\n",
    "    'ridge': Ridge()\n",
    "}\n",
    "\n",
    "# We aggregate all results here:\n",
    "df = pd.DataFrame(columns=['connectivity','model','target','n','r_discovery_cv','r_discovery_overfit','r_replication','p_discovery_cv','p_discovery_overfit','p_replication'])\n",
    "\n",
    "for feature_set in features:\n",
    "    for model in models:\n",
    "        for target_var in ['age', 'CogTotalComp_AgeAdj', 'PMAT24_A_CR', 'Flanker_AgeAdj', 'CardSort_AgeAdj', 'PicSeq_AgeAdj']:\n",
    "            for sample_size in [50, 100, 200, 300, 'max']:\n",
    "\n",
    "                print('*****************************************************************')\n",
    "                print(feature_set, model, target_var, sample_size)\n",
    "\n",
    "                X, y = create_data(target=target_var, feature_data=features[feature_set])\n",
    "\n",
    "                if sample_size=='max':\n",
    "                    sample_size = int(len(y)/2)\n",
    "\n",
    "                # create random seeds for each bootstrap iteration for reproducibility\n",
    "                rng = np.random.default_rng(random_state)\n",
    "                random_sates = rng.integers(np.iinfo(np.int32).max, size=n_bootstrap)\n",
    "\n",
    "                # run bootstrap iterations in parallel\n",
    "                r_discovery_cv, r_discovery_overfit, r_replication, p_discovery_cv, p_discovery_overfit, p_replication = zip(\n",
    "                    *Parallel(n_jobs=-1)(\n",
    "                    delayed(bootstrap_workhorse)(X, y, sample_size, models[model], seed) for seed in random_sates))\n",
    "\n",
    "                tmp_data_frame = pd.DataFrame({\n",
    "                    'connectivity' : feature_set,\n",
    "                    'model' : model,\n",
    "                    'target' : target_var,\n",
    "                    'n' : sample_size,\n",
    "                    'r_discovery_cv': r_discovery_cv,\n",
    "                    'r_discovery_overfit': r_discovery_overfit,\n",
    "                    'r_replication': r_replication,\n",
    "                    'p_discovery_cv': p_discovery_cv,\n",
    "                    'p_discovery_overfit': p_discovery_overfit,\n",
    "                    'p_replication': p_replication\n",
    "                })\n",
    "                #sns.scatterplot(x='r_replication', y='r_discovery_cv', data=tmp_data_frame)\n",
    "                #plt.ylabel('in-sample (r)')\n",
    "                #plt.xlabel('out-of-sample (r_pred)')\n",
    "                #plt.show()\n",
    "                print('r discovery (with cv) :', tmp_data_frame.r_discovery_cv.mean(), 'r replication:', tmp_data_frame.r_replication.mean())\n",
    "\n",
    "                for alpha in [0.05, 0.01, 0.005, 0.001]:\n",
    "                    print('Replicability at alpha =', alpha, ':',\n",
    "                          (tmp_data_frame.loc[tmp_data_frame['p_discovery_cv']<alpha,'p_replication']<alpha).sum() / (tmp_data_frame['p_discovery_cv']<0.05).sum() * 100, '%')\n",
    "\n",
    "                df = pd.concat((df, tmp_data_frame))\n",
    "                df.reset_index(drop=True, inplace=True)\n",
    "                df.to_csv('res/results_Ridge.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "data": {
      "text/plain": "                                               r_discovery_cv  r_replication\nconnectivity    model target              n                                 \nnetmats_parcor  ridge CardSort_AgeAdj     50         0.033223       0.090787\n                                          100        0.073555       0.101448\n                                          200        0.119109       0.142017\n                                          300        0.164847       0.167188\n                                          500        0.178966       0.189577\n                      CogTotalComp_AgeAdj 50         0.212834       0.256673\n                                          100        0.308142       0.324413\n                                          200        0.404524       0.406206\n                                          300        0.431054       0.443890\n                                          495        0.472652       0.479039\n                      Flanker_AgeAdj      50         0.044371       0.069322\n                                          100        0.073836       0.085089\n                                          200        0.104340       0.104674\n                                          300        0.117406       0.119512\n                                          501        0.132261       0.140226\n                      PMAT24_A_CR         50         0.182340       0.201273\n                                          100        0.226739       0.227487\n                                          200        0.272137       0.279880\n                                          300        0.286111       0.290239\n                                          499        0.292243       0.301492\n                      PicSeq_AgeAdj       50         0.106290       0.098604\n                                          100        0.094976       0.131433\n                                          200        0.146384       0.170334\n                                          300        0.170324       0.172413\n                                          501        0.182331       0.185931\n                      age                 50         0.242334       0.260920\n                                          100        0.332321       0.342876\n                                          200        0.395289       0.421317\n                                          300        0.441473       0.448254\n                                          501        0.480208       0.489671\nnetmats_pearson ridge CardSort_AgeAdj     50         0.052822       0.079214\n                                          100        0.077086       0.113478\n                                          200        0.114114       0.124521\n                                          300        0.135054       0.146406\n                                          500        0.143175       0.152942\n                      CogTotalComp_AgeAdj 50         0.134584       0.215036\n                                          100        0.246692       0.251511\n                                          200        0.300071       0.315974\n                                          300        0.335907       0.349330\n                                          495        0.371412       0.381920\n                      Flanker_AgeAdj      50         0.035977       0.057474\n                                          100        0.073144       0.072340\n                                          200        0.081574       0.088799\n                                          300        0.096957       0.099712\n                                          501        0.125087       0.119995\n                      PMAT24_A_CR         50         0.149342       0.138880\n                                          100        0.185662       0.183803\n                                          200        0.201747       0.222919\n                                          300        0.219045       0.232182\n                                          499        0.232732       0.243257\n                      PicSeq_AgeAdj       50         0.097874       0.094085\n                                          100        0.095827       0.124245\n                                          200        0.131418       0.138752\n                                          300        0.154384       0.159652\n                                          501        0.176843       0.189686\n                      age                 50         0.144368       0.184815\n                                          100        0.254658       0.268625\n                                          200        0.328774       0.368692\n                                          300        0.399369       0.405524\n                                          501        0.436267       0.441135",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th>r_discovery_cv</th>\n      <th>r_replication</th>\n    </tr>\n    <tr>\n      <th>connectivity</th>\n      <th>model</th>\n      <th>target</th>\n      <th>n</th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th rowspan=\"30\" valign=\"top\">netmats_parcor</th>\n      <th rowspan=\"30\" valign=\"top\">ridge</th>\n      <th rowspan=\"5\" valign=\"top\">CardSort_AgeAdj</th>\n      <th>50</th>\n      <td>0.033223</td>\n      <td>0.090787</td>\n    </tr>\n    <tr>\n      <th>100</th>\n      <td>0.073555</td>\n      <td>0.101448</td>\n    </tr>\n    <tr>\n      <th>200</th>\n      <td>0.119109</td>\n      <td>0.142017</td>\n    </tr>\n    <tr>\n      <th>300</th>\n      <td>0.164847</td>\n      <td>0.167188</td>\n    </tr>\n    <tr>\n      <th>500</th>\n      <td>0.178966</td>\n      <td>0.189577</td>\n    </tr>\n    <tr>\n      <th rowspan=\"5\" valign=\"top\">CogTotalComp_AgeAdj</th>\n      <th>50</th>\n      <td>0.212834</td>\n      <td>0.256673</td>\n    </tr>\n    <tr>\n      <th>100</th>\n      <td>0.308142</td>\n      <td>0.324413</td>\n    </tr>\n    <tr>\n      <th>200</th>\n      <td>0.404524</td>\n      <td>0.406206</td>\n    </tr>\n    <tr>\n      <th>300</th>\n      <td>0.431054</td>\n      <td>0.443890</td>\n    </tr>\n    <tr>\n      <th>495</th>\n      <td>0.472652</td>\n      <td>0.479039</td>\n    </tr>\n    <tr>\n      <th rowspan=\"5\" valign=\"top\">Flanker_AgeAdj</th>\n      <th>50</th>\n      <td>0.044371</td>\n      <td>0.069322</td>\n    </tr>\n    <tr>\n      <th>100</th>\n      <td>0.073836</td>\n      <td>0.085089</td>\n    </tr>\n    <tr>\n      <th>200</th>\n      <td>0.104340</td>\n      <td>0.104674</td>\n    </tr>\n    <tr>\n      <th>300</th>\n      <td>0.117406</td>\n      <td>0.119512</td>\n    </tr>\n    <tr>\n      <th>501</th>\n      <td>0.132261</td>\n      <td>0.140226</td>\n    </tr>\n    <tr>\n      <th rowspan=\"5\" valign=\"top\">PMAT24_A_CR</th>\n      <th>50</th>\n      <td>0.182340</td>\n      <td>0.201273</td>\n    </tr>\n    <tr>\n      <th>100</th>\n      <td>0.226739</td>\n      <td>0.227487</td>\n    </tr>\n    <tr>\n      <th>200</th>\n      <td>0.272137</td>\n      <td>0.279880</td>\n    </tr>\n    <tr>\n      <th>300</th>\n      <td>0.286111</td>\n      <td>0.290239</td>\n    </tr>\n    <tr>\n      <th>499</th>\n      <td>0.292243</td>\n      <td>0.301492</td>\n    </tr>\n    <tr>\n      <th rowspan=\"5\" valign=\"top\">PicSeq_AgeAdj</th>\n      <th>50</th>\n      <td>0.106290</td>\n      <td>0.098604</td>\n    </tr>\n    <tr>\n      <th>100</th>\n      <td>0.094976</td>\n      <td>0.131433</td>\n    </tr>\n    <tr>\n      <th>200</th>\n      <td>0.146384</td>\n      <td>0.170334</td>\n    </tr>\n    <tr>\n      <th>300</th>\n      <td>0.170324</td>\n      <td>0.172413</td>\n    </tr>\n    <tr>\n      <th>501</th>\n      <td>0.182331</td>\n      <td>0.185931</td>\n    </tr>\n    <tr>\n      <th rowspan=\"5\" valign=\"top\">age</th>\n      <th>50</th>\n      <td>0.242334</td>\n      <td>0.260920</td>\n    </tr>\n    <tr>\n      <th>100</th>\n      <td>0.332321</td>\n      <td>0.342876</td>\n    </tr>\n    <tr>\n      <th>200</th>\n      <td>0.395289</td>\n      <td>0.421317</td>\n    </tr>\n    <tr>\n      <th>300</th>\n      <td>0.441473</td>\n      <td>0.448254</td>\n    </tr>\n    <tr>\n      <th>501</th>\n      <td>0.480208</td>\n      <td>0.489671</td>\n    </tr>\n    <tr>\n      <th rowspan=\"30\" valign=\"top\">netmats_pearson</th>\n      <th rowspan=\"30\" valign=\"top\">ridge</th>\n      <th rowspan=\"5\" valign=\"top\">CardSort_AgeAdj</th>\n      <th>50</th>\n      <td>0.052822</td>\n      <td>0.079214</td>\n    </tr>\n    <tr>\n      <th>100</th>\n      <td>0.077086</td>\n      <td>0.113478</td>\n    </tr>\n    <tr>\n      <th>200</th>\n      <td>0.114114</td>\n      <td>0.124521</td>\n    </tr>\n    <tr>\n      <th>300</th>\n      <td>0.135054</td>\n      <td>0.146406</td>\n    </tr>\n    <tr>\n      <th>500</th>\n      <td>0.143175</td>\n      <td>0.152942</td>\n    </tr>\n    <tr>\n      <th rowspan=\"5\" valign=\"top\">CogTotalComp_AgeAdj</th>\n      <th>50</th>\n      <td>0.134584</td>\n      <td>0.215036</td>\n    </tr>\n    <tr>\n      <th>100</th>\n      <td>0.246692</td>\n      <td>0.251511</td>\n    </tr>\n    <tr>\n      <th>200</th>\n      <td>0.300071</td>\n      <td>0.315974</td>\n    </tr>\n    <tr>\n      <th>300</th>\n      <td>0.335907</td>\n      <td>0.349330</td>\n    </tr>\n    <tr>\n      <th>495</th>\n      <td>0.371412</td>\n      <td>0.381920</td>\n    </tr>\n    <tr>\n      <th rowspan=\"5\" valign=\"top\">Flanker_AgeAdj</th>\n      <th>50</th>\n      <td>0.035977</td>\n      <td>0.057474</td>\n    </tr>\n    <tr>\n      <th>100</th>\n      <td>0.073144</td>\n      <td>0.072340</td>\n    </tr>\n    <tr>\n      <th>200</th>\n      <td>0.081574</td>\n      <td>0.088799</td>\n    </tr>\n    <tr>\n      <th>300</th>\n      <td>0.096957</td>\n      <td>0.099712</td>\n    </tr>\n    <tr>\n      <th>501</th>\n      <td>0.125087</td>\n      <td>0.119995</td>\n    </tr>\n    <tr>\n      <th rowspan=\"5\" valign=\"top\">PMAT24_A_CR</th>\n      <th>50</th>\n      <td>0.149342</td>\n      <td>0.138880</td>\n    </tr>\n    <tr>\n      <th>100</th>\n      <td>0.185662</td>\n      <td>0.183803</td>\n    </tr>\n    <tr>\n      <th>200</th>\n      <td>0.201747</td>\n      <td>0.222919</td>\n    </tr>\n    <tr>\n      <th>300</th>\n      <td>0.219045</td>\n      <td>0.232182</td>\n    </tr>\n    <tr>\n      <th>499</th>\n      <td>0.232732</td>\n      <td>0.243257</td>\n    </tr>\n    <tr>\n      <th rowspan=\"5\" valign=\"top\">PicSeq_AgeAdj</th>\n      <th>50</th>\n      <td>0.097874</td>\n      <td>0.094085</td>\n    </tr>\n    <tr>\n      <th>100</th>\n      <td>0.095827</td>\n      <td>0.124245</td>\n    </tr>\n    <tr>\n      <th>200</th>\n      <td>0.131418</td>\n      <td>0.138752</td>\n    </tr>\n    <tr>\n      <th>300</th>\n      <td>0.154384</td>\n      <td>0.159652</td>\n    </tr>\n    <tr>\n      <th>501</th>\n      <td>0.176843</td>\n      <td>0.189686</td>\n    </tr>\n    <tr>\n      <th rowspan=\"5\" valign=\"top\">age</th>\n      <th>50</th>\n      <td>0.144368</td>\n      <td>0.184815</td>\n    </tr>\n    <tr>\n      <th>100</th>\n      <td>0.254658</td>\n      <td>0.268625</td>\n    </tr>\n    <tr>\n      <th>200</th>\n      <td>0.328774</td>\n      <td>0.368692</td>\n    </tr>\n    <tr>\n      <th>300</th>\n      <td>0.399369</td>\n      <td>0.405524</td>\n    </tr>\n    <tr>\n      <th>501</th>\n      <td>0.436267</td>\n      <td>0.441135</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby(['connectivity', 'model', 'target', 'n']).mean()[['r_discovery_cv', 'r_replication']]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Null scenario with random target\n",
    "To evaluate false positives with biased estimates (Figure 1d).\n",
    "\n",
    "First for PCA+SVR."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 46.5 s, sys: 6 s, total: 52.5 s\n",
      "Wall time: 3h 16min 32s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%%capture\n",
    "\n",
    "random_state = 42\n",
    "n_bootstrap = 100\n",
    "\n",
    "features = {\n",
    "    'netmats_parcor': netmats_parcor,\n",
    "    'netmats_pearson': netmats_pearson\n",
    "}\n",
    "\n",
    "models = {\n",
    "    'PCA_SVR': Pipeline([('pca', PCA(n_components=0.5)),\n",
    "                         ('svr', SVR())])\n",
    "\n",
    "}\n",
    "\n",
    "# We aggregate all results here:\n",
    "df = pd.DataFrame(columns=['connectivity','model','target','n','r_discovery_cv','r_discovery_overfit','r_replication','p_discovery_cv','p_discovery_overfit','p_replication'])\n",
    "\n",
    "for feature_set in features:\n",
    "    for model in models:\n",
    "        for target_var in ['age', 'CogTotalComp_AgeAdj', 'PMAT24_A_CR', 'Flanker_AgeAdj', 'CardSort_AgeAdj', 'PicSeq_AgeAdj']:\n",
    "            for sample_size in [50, 100, 200, 300, 'max']:\n",
    "\n",
    "                print('*****************************************************************')\n",
    "                print(feature_set, model, target_var, sample_size)\n",
    "\n",
    "                X, y = create_data(target=target_var, feature_data=features[feature_set]) # gives a random y when target is None\n",
    "\n",
    "                if sample_size=='max':\n",
    "                    sample_size = int(len(y)/2)\n",
    "\n",
    "                # create random seeds for each bootstrap iteration for reproducibility\n",
    "                rng = np.random.default_rng(random_state)\n",
    "                random_sates = rng.integers(np.iinfo(np.int32).max, size=n_bootstrap)\n",
    "\n",
    "                # run bootstrap iterations in parallel, with shuffle_y=True\n",
    "                r_discovery_cv, r_discovery_overfit, r_replication, p_discovery_cv, p_discovery_overfit, p_replication = zip(\n",
    "                    *Parallel(n_jobs=-1)(\n",
    "                    delayed(bootstrap_workhorse)(X, y, sample_size, models[model], seed, shuffle_y=True) for seed in random_sates))\n",
    "\n",
    "                tmp_data_frame = pd.DataFrame({\n",
    "                    'connectivity' : feature_set,\n",
    "                    'model' : model,\n",
    "                    'target' : target_var,\n",
    "                    'n' : sample_size,\n",
    "                    'r_discovery_cv': r_discovery_cv,\n",
    "                    'r_discovery_overfit': r_discovery_overfit,\n",
    "                    'r_replication': r_replication,\n",
    "                    'p_discovery_cv': p_discovery_cv,\n",
    "                    'p_discovery_overfit': p_discovery_overfit,\n",
    "                    'p_replication': p_replication\n",
    "                })\n",
    "\n",
    "                #sns.scatterplot(x='r_replication', y='r_discovery_cv', data=tmp_data_frame)\n",
    "                #plt.ylabel('in-sample (r)')\n",
    "                #plt.xlabel('out-of-sample (r_pred)')\n",
    "                #plt.show()\n",
    "                print('r discovery (with cv) :', tmp_data_frame.r_discovery_cv.mean(), 'r replication:', tmp_data_frame.r_replication.mean())\n",
    "\n",
    "                for alpha in [0.05, 0.01, 0.005, 0.001]:\n",
    "                    print('Replicability at alpha =', alpha, ':',\n",
    "                          (tmp_data_frame.loc[tmp_data_frame['p_discovery_cv']<alpha,'p_replication']<alpha).sum() / (tmp_data_frame['p_discovery_cv']<0.05).sum() * 100, '%')\n",
    "\n",
    "                df = pd.concat((df, tmp_data_frame))\n",
    "                df.reset_index(drop=True, inplace=True)\n",
    "                df.to_csv('res/results_null_PCA_SVR.csv')\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "data": {
      "text/plain": "                                                 r_discovery_cv  r_replication\nconnectivity    model   target              n                                 \nnetmats_parcor  PCA_SVR CardSort_AgeAdj     50         0.028683       0.002786\n                                            100        0.006824       0.011154\n                                            200       -0.011921       0.004941\n                                            300        0.000561      -0.003558\n                                            500       -0.002693       0.004389\n                        CogTotalComp_AgeAdj 50        -0.001601       0.004007\n                                            100        0.005333      -0.012090\n                                            200       -0.001798      -0.012032\n                                            300        0.008406      -0.009813\n                                            495       -0.007775      -0.002075\n                        Flanker_AgeAdj      50         0.000604       0.007010\n                                            100       -0.003850      -0.008456\n                                            200       -0.005590       0.006889\n                                            300       -0.006700       0.002094\n                                            501       -0.001520       0.001972\n                        PMAT24_A_CR         50         0.007739      -0.020072\n                                            100        0.013193      -0.013371\n                                            200        0.001066       0.000112\n                                            300        0.017003      -0.009327\n                                            499       -0.002256      -0.010670\n                        PicSeq_AgeAdj       50        -0.024504       0.004879\n                                            100        0.006898       0.001946\n                                            200       -0.015076       0.002704\n                                            300        0.012178      -0.005959\n                                            501       -0.009452       0.002955\n                        age                 50         0.015718      -0.009632\n                                            100        0.021183       0.000831\n                                            200        0.002010      -0.002066\n                                            300        0.010896      -0.014503\n                                            501        0.007978      -0.001730\nnetmats_pearson PCA_SVR CardSort_AgeAdj     50        -0.006625       0.005565\n                                            100       -0.006670      -0.001255\n                                            200       -0.007012      -0.001359\n                                            300        0.014291      -0.000438\n                                            500        0.000080       0.000920\n                        CogTotalComp_AgeAdj 50         0.018770      -0.020559\n                                            100        0.008058       0.000762\n                                            200       -0.009799      -0.000800\n                                            300       -0.000056       0.004032\n                                            495       -0.004324       0.000941\n                        Flanker_AgeAdj      50        -0.006951       0.013101\n                                            100        0.002655      -0.011287\n                                            200        0.003231       0.010932\n                                            300       -0.009365       0.004417\n                                            501       -0.008707       0.004297\n                        PMAT24_A_CR         50         0.008029      -0.007791\n                                            100        0.017448      -0.011428\n                                            200        0.007621       0.007601\n                                            300        0.013826       0.000286\n                                            499        0.001244       0.000679\n                        PicSeq_AgeAdj       50         0.017904       0.014805\n                                            100       -0.006270       0.001866\n                                            200       -0.005345       0.001373\n                                            300        0.010393       0.000526\n                                            501       -0.000391       0.001613\n                        age                 50         0.024571      -0.014452\n                                            100       -0.000449       0.030406\n                                            200       -0.001005       0.003157\n                                            300        0.009528      -0.008432\n                                            501        0.001958       0.003078",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th>r_discovery_cv</th>\n      <th>r_replication</th>\n    </tr>\n    <tr>\n      <th>connectivity</th>\n      <th>model</th>\n      <th>target</th>\n      <th>n</th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th rowspan=\"30\" valign=\"top\">netmats_parcor</th>\n      <th rowspan=\"30\" valign=\"top\">PCA_SVR</th>\n      <th rowspan=\"5\" valign=\"top\">CardSort_AgeAdj</th>\n      <th>50</th>\n      <td>0.028683</td>\n      <td>0.002786</td>\n    </tr>\n    <tr>\n      <th>100</th>\n      <td>0.006824</td>\n      <td>0.011154</td>\n    </tr>\n    <tr>\n      <th>200</th>\n      <td>-0.011921</td>\n      <td>0.004941</td>\n    </tr>\n    <tr>\n      <th>300</th>\n      <td>0.000561</td>\n      <td>-0.003558</td>\n    </tr>\n    <tr>\n      <th>500</th>\n      <td>-0.002693</td>\n      <td>0.004389</td>\n    </tr>\n    <tr>\n      <th rowspan=\"5\" valign=\"top\">CogTotalComp_AgeAdj</th>\n      <th>50</th>\n      <td>-0.001601</td>\n      <td>0.004007</td>\n    </tr>\n    <tr>\n      <th>100</th>\n      <td>0.005333</td>\n      <td>-0.012090</td>\n    </tr>\n    <tr>\n      <th>200</th>\n      <td>-0.001798</td>\n      <td>-0.012032</td>\n    </tr>\n    <tr>\n      <th>300</th>\n      <td>0.008406</td>\n      <td>-0.009813</td>\n    </tr>\n    <tr>\n      <th>495</th>\n      <td>-0.007775</td>\n      <td>-0.002075</td>\n    </tr>\n    <tr>\n      <th rowspan=\"5\" valign=\"top\">Flanker_AgeAdj</th>\n      <th>50</th>\n      <td>0.000604</td>\n      <td>0.007010</td>\n    </tr>\n    <tr>\n      <th>100</th>\n      <td>-0.003850</td>\n      <td>-0.008456</td>\n    </tr>\n    <tr>\n      <th>200</th>\n      <td>-0.005590</td>\n      <td>0.006889</td>\n    </tr>\n    <tr>\n      <th>300</th>\n      <td>-0.006700</td>\n      <td>0.002094</td>\n    </tr>\n    <tr>\n      <th>501</th>\n      <td>-0.001520</td>\n      <td>0.001972</td>\n    </tr>\n    <tr>\n      <th rowspan=\"5\" valign=\"top\">PMAT24_A_CR</th>\n      <th>50</th>\n      <td>0.007739</td>\n      <td>-0.020072</td>\n    </tr>\n    <tr>\n      <th>100</th>\n      <td>0.013193</td>\n      <td>-0.013371</td>\n    </tr>\n    <tr>\n      <th>200</th>\n      <td>0.001066</td>\n      <td>0.000112</td>\n    </tr>\n    <tr>\n      <th>300</th>\n      <td>0.017003</td>\n      <td>-0.009327</td>\n    </tr>\n    <tr>\n      <th>499</th>\n      <td>-0.002256</td>\n      <td>-0.010670</td>\n    </tr>\n    <tr>\n      <th rowspan=\"5\" valign=\"top\">PicSeq_AgeAdj</th>\n      <th>50</th>\n      <td>-0.024504</td>\n      <td>0.004879</td>\n    </tr>\n    <tr>\n      <th>100</th>\n      <td>0.006898</td>\n      <td>0.001946</td>\n    </tr>\n    <tr>\n      <th>200</th>\n      <td>-0.015076</td>\n      <td>0.002704</td>\n    </tr>\n    <tr>\n      <th>300</th>\n      <td>0.012178</td>\n      <td>-0.005959</td>\n    </tr>\n    <tr>\n      <th>501</th>\n      <td>-0.009452</td>\n      <td>0.002955</td>\n    </tr>\n    <tr>\n      <th rowspan=\"5\" valign=\"top\">age</th>\n      <th>50</th>\n      <td>0.015718</td>\n      <td>-0.009632</td>\n    </tr>\n    <tr>\n      <th>100</th>\n      <td>0.021183</td>\n      <td>0.000831</td>\n    </tr>\n    <tr>\n      <th>200</th>\n      <td>0.002010</td>\n      <td>-0.002066</td>\n    </tr>\n    <tr>\n      <th>300</th>\n      <td>0.010896</td>\n      <td>-0.014503</td>\n    </tr>\n    <tr>\n      <th>501</th>\n      <td>0.007978</td>\n      <td>-0.001730</td>\n    </tr>\n    <tr>\n      <th rowspan=\"30\" valign=\"top\">netmats_pearson</th>\n      <th rowspan=\"30\" valign=\"top\">PCA_SVR</th>\n      <th rowspan=\"5\" valign=\"top\">CardSort_AgeAdj</th>\n      <th>50</th>\n      <td>-0.006625</td>\n      <td>0.005565</td>\n    </tr>\n    <tr>\n      <th>100</th>\n      <td>-0.006670</td>\n      <td>-0.001255</td>\n    </tr>\n    <tr>\n      <th>200</th>\n      <td>-0.007012</td>\n      <td>-0.001359</td>\n    </tr>\n    <tr>\n      <th>300</th>\n      <td>0.014291</td>\n      <td>-0.000438</td>\n    </tr>\n    <tr>\n      <th>500</th>\n      <td>0.000080</td>\n      <td>0.000920</td>\n    </tr>\n    <tr>\n      <th rowspan=\"5\" valign=\"top\">CogTotalComp_AgeAdj</th>\n      <th>50</th>\n      <td>0.018770</td>\n      <td>-0.020559</td>\n    </tr>\n    <tr>\n      <th>100</th>\n      <td>0.008058</td>\n      <td>0.000762</td>\n    </tr>\n    <tr>\n      <th>200</th>\n      <td>-0.009799</td>\n      <td>-0.000800</td>\n    </tr>\n    <tr>\n      <th>300</th>\n      <td>-0.000056</td>\n      <td>0.004032</td>\n    </tr>\n    <tr>\n      <th>495</th>\n      <td>-0.004324</td>\n      <td>0.000941</td>\n    </tr>\n    <tr>\n      <th rowspan=\"5\" valign=\"top\">Flanker_AgeAdj</th>\n      <th>50</th>\n      <td>-0.006951</td>\n      <td>0.013101</td>\n    </tr>\n    <tr>\n      <th>100</th>\n      <td>0.002655</td>\n      <td>-0.011287</td>\n    </tr>\n    <tr>\n      <th>200</th>\n      <td>0.003231</td>\n      <td>0.010932</td>\n    </tr>\n    <tr>\n      <th>300</th>\n      <td>-0.009365</td>\n      <td>0.004417</td>\n    </tr>\n    <tr>\n      <th>501</th>\n      <td>-0.008707</td>\n      <td>0.004297</td>\n    </tr>\n    <tr>\n      <th rowspan=\"5\" valign=\"top\">PMAT24_A_CR</th>\n      <th>50</th>\n      <td>0.008029</td>\n      <td>-0.007791</td>\n    </tr>\n    <tr>\n      <th>100</th>\n      <td>0.017448</td>\n      <td>-0.011428</td>\n    </tr>\n    <tr>\n      <th>200</th>\n      <td>0.007621</td>\n      <td>0.007601</td>\n    </tr>\n    <tr>\n      <th>300</th>\n      <td>0.013826</td>\n      <td>0.000286</td>\n    </tr>\n    <tr>\n      <th>499</th>\n      <td>0.001244</td>\n      <td>0.000679</td>\n    </tr>\n    <tr>\n      <th rowspan=\"5\" valign=\"top\">PicSeq_AgeAdj</th>\n      <th>50</th>\n      <td>0.017904</td>\n      <td>0.014805</td>\n    </tr>\n    <tr>\n      <th>100</th>\n      <td>-0.006270</td>\n      <td>0.001866</td>\n    </tr>\n    <tr>\n      <th>200</th>\n      <td>-0.005345</td>\n      <td>0.001373</td>\n    </tr>\n    <tr>\n      <th>300</th>\n      <td>0.010393</td>\n      <td>0.000526</td>\n    </tr>\n    <tr>\n      <th>501</th>\n      <td>-0.000391</td>\n      <td>0.001613</td>\n    </tr>\n    <tr>\n      <th rowspan=\"5\" valign=\"top\">age</th>\n      <th>50</th>\n      <td>0.024571</td>\n      <td>-0.014452</td>\n    </tr>\n    <tr>\n      <th>100</th>\n      <td>-0.000449</td>\n      <td>0.030406</td>\n    </tr>\n    <tr>\n      <th>200</th>\n      <td>-0.001005</td>\n      <td>0.003157</td>\n    </tr>\n    <tr>\n      <th>300</th>\n      <td>0.009528</td>\n      <td>-0.008432</td>\n    </tr>\n    <tr>\n      <th>501</th>\n      <td>0.001958</td>\n      <td>0.003078</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby(['connectivity', 'model', 'target', 'n']).mean()[['r_discovery_cv', 'r_replication']]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then with Ridge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 41.3 s, sys: 5.51 s, total: 46.8 s\n",
      "Wall time: 26min 40s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%%capture\n",
    "\n",
    "random_state = 42\n",
    "n_bootstrap = 100\n",
    "\n",
    "features = {\n",
    "    'netmats_parcor': netmats_parcor,\n",
    "    'netmats_pearson': netmats_pearson\n",
    "}\n",
    "\n",
    "models = {\n",
    "    'Ridge': Ridge()\n",
    "\n",
    "}\n",
    "\n",
    "# We aggregate all results here:\n",
    "df = pd.DataFrame(columns=['connectivity','model','target','n','r_discovery_cv','r_discovery_overfit','r_replication','p_discovery_cv','p_discovery_overfit','p_replication'])\n",
    "\n",
    "for feature_set in features:\n",
    "    for model in models:\n",
    "        for target_var in ['age', 'CogTotalComp_AgeAdj', 'PMAT24_A_CR', 'Flanker_AgeAdj', 'CardSort_AgeAdj', 'PicSeq_AgeAdj']:\n",
    "            for sample_size in [50, 100, 200, 300, 'max']:\n",
    "\n",
    "                print('*****************************************************************')\n",
    "                print(feature_set, model, target_var, sample_size)\n",
    "\n",
    "                X, y = create_data(target=target_var, feature_data=features[feature_set]) # gives a random y when target is None\n",
    "\n",
    "                if sample_size=='max':\n",
    "                    sample_size = int(len(y)/2)\n",
    "\n",
    "                # create random seeds for each bootstrap iteration for reproducibility\n",
    "                rng = np.random.default_rng(random_state)\n",
    "                random_sates = rng.integers(np.iinfo(np.int32).max, size=n_bootstrap)\n",
    "\n",
    "                # run bootstrap iterations in parallel, with shuffle_y=True\n",
    "                r_discovery_cv, r_discovery_overfit, r_replication, p_discovery_cv, p_discovery_overfit, p_replication = zip(\n",
    "                    *Parallel(n_jobs=-1)(\n",
    "                    delayed(bootstrap_workhorse)(X, y, sample_size, models[model], seed, shuffle_y=True) for seed in random_sates))\n",
    "\n",
    "                tmp_data_frame = pd.DataFrame({\n",
    "                    'connectivity' : feature_set,\n",
    "                    'model' : model,\n",
    "                    'target' : target_var,\n",
    "                    'n' : sample_size,\n",
    "                    'r_discovery_cv': r_discovery_cv,\n",
    "                    'r_discovery_overfit': r_discovery_overfit,\n",
    "                    'r_replication': r_replication,\n",
    "                    'p_discovery_cv': p_discovery_cv,\n",
    "                    'p_discovery_overfit': p_discovery_overfit,\n",
    "                    'p_replication': p_replication\n",
    "                })\n",
    "\n",
    "                #sns.scatterplot(x='r_replication', y='r_discovery_cv', data=tmp_data_frame)\n",
    "                #plt.ylabel('in-sample (r)')\n",
    "                #plt.xlabel('out-of-sample (r_pred)')\n",
    "                #plt.show()\n",
    "                print('r discovery (with cv) :', tmp_data_frame.r_discovery_cv.mean(), 'r replication:', tmp_data_frame.r_replication.mean())\n",
    "\n",
    "                for alpha in [0.05, 0.01, 0.005, 0.001]:\n",
    "                    print('Replicability at alpha =', alpha, ':',\n",
    "                          (tmp_data_frame.loc[tmp_data_frame['p_discovery_cv']<alpha,'p_replication']<alpha).sum() / (tmp_data_frame['p_discovery_cv']<0.05).sum() * 100, '%')\n",
    "\n",
    "                df = pd.concat((df, tmp_data_frame))\n",
    "                df.reset_index(drop=True, inplace=True)\n",
    "                df.to_csv('res/results_null_Ridge.csv')\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "data": {
      "text/plain": "                                               r_discovery_cv  r_replication\nconnectivity    model target              n                                 \nnetmats_parcor  Ridge CardSort_AgeAdj     50         0.031644      -0.000121\n                                          100        0.007479       0.014062\n                                          200       -0.020488       0.010242\n                                          300       -0.002089      -0.001656\n                                          500       -0.000114      -0.001920\n                      CogTotalComp_AgeAdj 50         0.022939      -0.003553\n                                          100       -0.002252      -0.007803\n                                          200       -0.017407       0.008146\n                                          300       -0.009672      -0.005715\n                                          495        0.008420      -0.001433\n                      Flanker_AgeAdj      50        -0.005864       0.007939\n                                          100        0.005748      -0.001361\n                                          200        0.004708       0.004419\n                                          300       -0.016153       0.006954\n                                          501        0.009223       0.003327\n                      PMAT24_A_CR         50         0.002180      -0.021616\n                                          100       -0.003001      -0.012718\n                                          200        0.000105       0.003351\n                                          300       -0.012994      -0.005839\n                                          499       -0.002584      -0.002607\n                      PicSeq_AgeAdj       50        -0.027364      -0.012612\n                                          100        0.021624      -0.001677\n                                          200        0.011211       0.001994\n                                          300        0.001884      -0.003351\n                                          501        0.002581      -0.004661\n                      age                 50        -0.014349      -0.019866\n                                          100        0.011020       0.008189\n                                          200        0.014167      -0.004365\n                                          300       -0.002140      -0.003427\n                                          501       -0.000934       0.007506\nnetmats_pearson Ridge CardSort_AgeAdj     50         0.032022      -0.035335\n                                          100       -0.013893      -0.005578\n                                          200       -0.003955      -0.001402\n                                          300        0.000762      -0.002684\n                                          500        0.001026       0.004724\n                      CogTotalComp_AgeAdj 50         0.044520       0.004646\n                                          100       -0.013345       0.001725\n                                          200       -0.005666       0.010043\n                                          300        0.004127      -0.000221\n                                          495        0.002043      -0.005491\n                      Flanker_AgeAdj      50        -0.007542       0.003721\n                                          100        0.001752       0.005816\n                                          200       -0.002066       0.013489\n                                          300       -0.019641       0.006051\n                                          501        0.003926      -0.002151\n                      PMAT24_A_CR         50        -0.002403       0.003701\n                                          100       -0.012464      -0.014601\n                                          200        0.000571      -0.001337\n                                          300        0.001551      -0.005956\n                                          499        0.002485      -0.005751\n                      PicSeq_AgeAdj       50         0.005861      -0.012902\n                                          100       -0.005922       0.001395\n                                          200       -0.014861      -0.002965\n                                          300       -0.003700      -0.005644\n                                          501       -0.001911      -0.000418\n                      age                 50        -0.006906      -0.020845\n                                          100        0.001887       0.009290\n                                          200       -0.005734       0.001369\n                                          300        0.005959      -0.000003\n                                          501       -0.004047       0.008781",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th>r_discovery_cv</th>\n      <th>r_replication</th>\n    </tr>\n    <tr>\n      <th>connectivity</th>\n      <th>model</th>\n      <th>target</th>\n      <th>n</th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th rowspan=\"30\" valign=\"top\">netmats_parcor</th>\n      <th rowspan=\"30\" valign=\"top\">Ridge</th>\n      <th rowspan=\"5\" valign=\"top\">CardSort_AgeAdj</th>\n      <th>50</th>\n      <td>0.031644</td>\n      <td>-0.000121</td>\n    </tr>\n    <tr>\n      <th>100</th>\n      <td>0.007479</td>\n      <td>0.014062</td>\n    </tr>\n    <tr>\n      <th>200</th>\n      <td>-0.020488</td>\n      <td>0.010242</td>\n    </tr>\n    <tr>\n      <th>300</th>\n      <td>-0.002089</td>\n      <td>-0.001656</td>\n    </tr>\n    <tr>\n      <th>500</th>\n      <td>-0.000114</td>\n      <td>-0.001920</td>\n    </tr>\n    <tr>\n      <th rowspan=\"5\" valign=\"top\">CogTotalComp_AgeAdj</th>\n      <th>50</th>\n      <td>0.022939</td>\n      <td>-0.003553</td>\n    </tr>\n    <tr>\n      <th>100</th>\n      <td>-0.002252</td>\n      <td>-0.007803</td>\n    </tr>\n    <tr>\n      <th>200</th>\n      <td>-0.017407</td>\n      <td>0.008146</td>\n    </tr>\n    <tr>\n      <th>300</th>\n      <td>-0.009672</td>\n      <td>-0.005715</td>\n    </tr>\n    <tr>\n      <th>495</th>\n      <td>0.008420</td>\n      <td>-0.001433</td>\n    </tr>\n    <tr>\n      <th rowspan=\"5\" valign=\"top\">Flanker_AgeAdj</th>\n      <th>50</th>\n      <td>-0.005864</td>\n      <td>0.007939</td>\n    </tr>\n    <tr>\n      <th>100</th>\n      <td>0.005748</td>\n      <td>-0.001361</td>\n    </tr>\n    <tr>\n      <th>200</th>\n      <td>0.004708</td>\n      <td>0.004419</td>\n    </tr>\n    <tr>\n      <th>300</th>\n      <td>-0.016153</td>\n      <td>0.006954</td>\n    </tr>\n    <tr>\n      <th>501</th>\n      <td>0.009223</td>\n      <td>0.003327</td>\n    </tr>\n    <tr>\n      <th rowspan=\"5\" valign=\"top\">PMAT24_A_CR</th>\n      <th>50</th>\n      <td>0.002180</td>\n      <td>-0.021616</td>\n    </tr>\n    <tr>\n      <th>100</th>\n      <td>-0.003001</td>\n      <td>-0.012718</td>\n    </tr>\n    <tr>\n      <th>200</th>\n      <td>0.000105</td>\n      <td>0.003351</td>\n    </tr>\n    <tr>\n      <th>300</th>\n      <td>-0.012994</td>\n      <td>-0.005839</td>\n    </tr>\n    <tr>\n      <th>499</th>\n      <td>-0.002584</td>\n      <td>-0.002607</td>\n    </tr>\n    <tr>\n      <th rowspan=\"5\" valign=\"top\">PicSeq_AgeAdj</th>\n      <th>50</th>\n      <td>-0.027364</td>\n      <td>-0.012612</td>\n    </tr>\n    <tr>\n      <th>100</th>\n      <td>0.021624</td>\n      <td>-0.001677</td>\n    </tr>\n    <tr>\n      <th>200</th>\n      <td>0.011211</td>\n      <td>0.001994</td>\n    </tr>\n    <tr>\n      <th>300</th>\n      <td>0.001884</td>\n      <td>-0.003351</td>\n    </tr>\n    <tr>\n      <th>501</th>\n      <td>0.002581</td>\n      <td>-0.004661</td>\n    </tr>\n    <tr>\n      <th rowspan=\"5\" valign=\"top\">age</th>\n      <th>50</th>\n      <td>-0.014349</td>\n      <td>-0.019866</td>\n    </tr>\n    <tr>\n      <th>100</th>\n      <td>0.011020</td>\n      <td>0.008189</td>\n    </tr>\n    <tr>\n      <th>200</th>\n      <td>0.014167</td>\n      <td>-0.004365</td>\n    </tr>\n    <tr>\n      <th>300</th>\n      <td>-0.002140</td>\n      <td>-0.003427</td>\n    </tr>\n    <tr>\n      <th>501</th>\n      <td>-0.000934</td>\n      <td>0.007506</td>\n    </tr>\n    <tr>\n      <th rowspan=\"30\" valign=\"top\">netmats_pearson</th>\n      <th rowspan=\"30\" valign=\"top\">Ridge</th>\n      <th rowspan=\"5\" valign=\"top\">CardSort_AgeAdj</th>\n      <th>50</th>\n      <td>0.032022</td>\n      <td>-0.035335</td>\n    </tr>\n    <tr>\n      <th>100</th>\n      <td>-0.013893</td>\n      <td>-0.005578</td>\n    </tr>\n    <tr>\n      <th>200</th>\n      <td>-0.003955</td>\n      <td>-0.001402</td>\n    </tr>\n    <tr>\n      <th>300</th>\n      <td>0.000762</td>\n      <td>-0.002684</td>\n    </tr>\n    <tr>\n      <th>500</th>\n      <td>0.001026</td>\n      <td>0.004724</td>\n    </tr>\n    <tr>\n      <th rowspan=\"5\" valign=\"top\">CogTotalComp_AgeAdj</th>\n      <th>50</th>\n      <td>0.044520</td>\n      <td>0.004646</td>\n    </tr>\n    <tr>\n      <th>100</th>\n      <td>-0.013345</td>\n      <td>0.001725</td>\n    </tr>\n    <tr>\n      <th>200</th>\n      <td>-0.005666</td>\n      <td>0.010043</td>\n    </tr>\n    <tr>\n      <th>300</th>\n      <td>0.004127</td>\n      <td>-0.000221</td>\n    </tr>\n    <tr>\n      <th>495</th>\n      <td>0.002043</td>\n      <td>-0.005491</td>\n    </tr>\n    <tr>\n      <th rowspan=\"5\" valign=\"top\">Flanker_AgeAdj</th>\n      <th>50</th>\n      <td>-0.007542</td>\n      <td>0.003721</td>\n    </tr>\n    <tr>\n      <th>100</th>\n      <td>0.001752</td>\n      <td>0.005816</td>\n    </tr>\n    <tr>\n      <th>200</th>\n      <td>-0.002066</td>\n      <td>0.013489</td>\n    </tr>\n    <tr>\n      <th>300</th>\n      <td>-0.019641</td>\n      <td>0.006051</td>\n    </tr>\n    <tr>\n      <th>501</th>\n      <td>0.003926</td>\n      <td>-0.002151</td>\n    </tr>\n    <tr>\n      <th rowspan=\"5\" valign=\"top\">PMAT24_A_CR</th>\n      <th>50</th>\n      <td>-0.002403</td>\n      <td>0.003701</td>\n    </tr>\n    <tr>\n      <th>100</th>\n      <td>-0.012464</td>\n      <td>-0.014601</td>\n    </tr>\n    <tr>\n      <th>200</th>\n      <td>0.000571</td>\n      <td>-0.001337</td>\n    </tr>\n    <tr>\n      <th>300</th>\n      <td>0.001551</td>\n      <td>-0.005956</td>\n    </tr>\n    <tr>\n      <th>499</th>\n      <td>0.002485</td>\n      <td>-0.005751</td>\n    </tr>\n    <tr>\n      <th rowspan=\"5\" valign=\"top\">PicSeq_AgeAdj</th>\n      <th>50</th>\n      <td>0.005861</td>\n      <td>-0.012902</td>\n    </tr>\n    <tr>\n      <th>100</th>\n      <td>-0.005922</td>\n      <td>0.001395</td>\n    </tr>\n    <tr>\n      <th>200</th>\n      <td>-0.014861</td>\n      <td>-0.002965</td>\n    </tr>\n    <tr>\n      <th>300</th>\n      <td>-0.003700</td>\n      <td>-0.005644</td>\n    </tr>\n    <tr>\n      <th>501</th>\n      <td>-0.001911</td>\n      <td>-0.000418</td>\n    </tr>\n    <tr>\n      <th rowspan=\"5\" valign=\"top\">age</th>\n      <th>50</th>\n      <td>-0.006906</td>\n      <td>-0.020845</td>\n    </tr>\n    <tr>\n      <th>100</th>\n      <td>0.001887</td>\n      <td>0.009290</td>\n    </tr>\n    <tr>\n      <th>200</th>\n      <td>-0.005734</td>\n      <td>0.001369</td>\n    </tr>\n    <tr>\n      <th>300</th>\n      <td>0.005959</td>\n      <td>-0.000003</td>\n    </tr>\n    <tr>\n      <th>501</th>\n      <td>-0.004047</td>\n      <td>0.008781</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby(['connectivity', 'model', 'target', 'n']).mean()[['r_discovery_cv', 'r_replication']]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*See the notebook called 'plot_results.ipynb' for the results.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now we train on the whole dataset, to obtain effect size estimates on N=1000.\n",
    "\n",
    "With PCA+SVR:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "age\n",
      "r = 0.21 \tp = 0.001 \tR2 = 4.2 %\n",
      "CogTotalComp_AgeAdj\n",
      "r = 0.2 \tp = 0.001 \tR2 = 3.9 %\n",
      "PMAT24_A_CR\n",
      "r = 0.21 \tp = 0.001 \tR2 = 4.4 %\n",
      "Flanker_AgeAdj\n",
      "r = 0.12 \tp = 0.001 \tR2 = 1.5 %\n",
      "CardSort_AgeAdj\n",
      "r = 0.15 \tp = 0.001 \tR2 = 2.1 %\n",
      "PicSeq_AgeAdj\n",
      "r = 0.15 \tp = 0.001 \tR2 = 2.2 %\n"
     ]
    }
   ],
   "source": [
    "model = Pipeline([('pca', PCA(n_components=0.5)), ('svr', SVR())])\n",
    "random_state = 42\n",
    "cv = KFold(10, shuffle=True, random_state=random_state)\n",
    "\n",
    "bar_data_svr = []\n",
    "\n",
    "for target_var in ['age', 'CogTotalComp_AgeAdj', 'PMAT24_A_CR', 'Flanker_AgeAdj', 'CardSort_AgeAdj', 'PicSeq_AgeAdj']:\n",
    "    print(target_var)\n",
    "    X, y = create_data(target=target_var, feature_data=netmats_pearson)\n",
    "\n",
    "    predicted_discovery_cv = np.zeros_like(y)\n",
    "    cor_per_fold = np.zeros(cv.n_splits)\n",
    "    i = 0\n",
    "    for train, test in cv.split(X=X, y=y):\n",
    "        model.fit(X=X[train], y=y[train])\n",
    "        predicted_discovery_cv[test] = model.predict(X=X[test])\n",
    "        cor_per_fold[i] = np.corrcoef(y[test], predicted_discovery_cv[test])[0,1]\n",
    "        i += 1\n",
    "    # correlation between the cross-validated predictions and observations in the discovery sample\n",
    "    # this is the correct, unbiased estimate!\n",
    "    # calculated as mean test performance across all folds\n",
    "    r_disc_cv = np.mean(cor_per_fold)\n",
    "    # finalize model by training it on the full discovery sample (without cross-validation)\n",
    "    final_model = model.fit(X=X, y=y)\n",
    "    # obtain predictions with the final model on the discovery sample, note that this model actually overfits this sample.\n",
    "    # we do this only to demonstrate biased estimates\n",
    "    predicted_discovery_overfit = final_model.predict(X=X)\n",
    "    # here we obtain the biased effect size (r) estimates for demonstrational purposes\n",
    "    r_disc_overfit = np.corrcoef(predicted_discovery_overfit, y)[0, 1]\n",
    "\n",
    "    # below we calculate permutation-based p-values for all three effect size estimates (in-sample unbiased, in-sample biased, out-of-sample)\n",
    "    # (one sided tests, testing for positive correlation)\n",
    "    p_disc_cv = permutation_test(predicted_discovery_cv, y, method='approximate', num_rounds=1000, func=lambda x, y: np.corrcoef(x, y)[1][0],seed=random_state)\n",
    "    p_disc_overfit = permutation_test(predicted_discovery_overfit, y, method='approximate', num_rounds=1000, func=lambda x, y: np.corrcoef(x, y)[1][0],seed=random_state)\n",
    "\n",
    "    bar_data_svr.append(r_disc_cv)\n",
    "\n",
    "    print('r =', np.round(r_disc_cv, 2), '\\tp =', np.round(p_disc_cv, 3), '\\tR2 =', np.round(r_disc_cv**2 * 100, 1),  '%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With Ridge and Pearson correlation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "age\n",
      "r = 0.45 \tp = 0.001 \tR2 = 20.0 %\n",
      "CogTotalComp_AgeAdj\n",
      "r = 0.4 \tp = 0.001 \tR2 = 16.2 %\n",
      "PMAT24_A_CR\n",
      "r = 0.25 \tp = 0.001 \tR2 = 6.3 %\n",
      "Flanker_AgeAdj\n",
      "r = 0.16 \tp = 0.001 \tR2 = 2.6 %\n",
      "CardSort_AgeAdj\n",
      "r = 0.17 \tp = 0.001 \tR2 = 2.8 %\n",
      "PicSeq_AgeAdj\n",
      "r = 0.23 \tp = 0.001 \tR2 = 5.5 %\n"
     ]
    }
   ],
   "source": [
    "model = Ridge()\n",
    "random_state = 42\n",
    "cv = KFold(10, shuffle=True, random_state=random_state)\n",
    "\n",
    "for target_var in ['age', 'CogTotalComp_AgeAdj', 'PMAT24_A_CR', 'Flanker_AgeAdj', 'CardSort_AgeAdj', 'PicSeq_AgeAdj']:\n",
    "    print(target_var)\n",
    "    X, y = create_data(target=target_var, feature_data=netmats_pearson)\n",
    "\n",
    "    predicted_discovery_cv = np.zeros_like(y)\n",
    "    cor_per_fold = np.zeros(cv.n_splits)\n",
    "    i = 0\n",
    "    for train, test in cv.split(X=X, y=y):\n",
    "        model.fit(X=X[train], y=y[train])\n",
    "        predicted_discovery_cv[test] = model.predict(X=X[test])\n",
    "        cor_per_fold[i] = np.corrcoef(y[test], predicted_discovery_cv[test])[0,1]\n",
    "        i += 1\n",
    "    # correlation between the cross-validated predictions and observations in the discovery sample\n",
    "    # this is the correct, unbiased estimate!\n",
    "    # calculated as mean test performance across all folds\n",
    "    r_disc_cv = np.mean(cor_per_fold)\n",
    "    # finalize model by training it on the full discovery sample (without cross-validation)\n",
    "    final_model = model.fit(X=X, y=y)\n",
    "    # obtain predictions with the final model on the discovery sample, note that this model actually overfits this sample.\n",
    "    # we do this only to demonstrate biased estimates\n",
    "    predicted_discovery_overfit = final_model.predict(X=X)\n",
    "    # here we obtain the biased effect size (r) estimates for demonstrational purposes\n",
    "    r_disc_overfit = np.corrcoef(predicted_discovery_overfit, y)[0, 1]\n",
    "\n",
    "    # below we calculate permutation-based p-values for all three effect size estimates (in-sample unbiased, in-sample biased, out-of-sample)\n",
    "    # (one sided tests, testing for positive correlation)\n",
    "    p_disc_cv = permutation_test(predicted_discovery_cv, y, method='approximate', num_rounds=1000, func=lambda x, y: np.corrcoef(x, y)[1][0],seed=random_state)\n",
    "    p_disc_overfit = permutation_test(predicted_discovery_overfit, y, method='approximate', num_rounds=1000, func=lambda x, y: np.corrcoef(x, y)[1][0],seed=random_state)\n",
    "\n",
    "    print('r =', np.round(r_disc_cv, 2), '\\tp =', np.round(p_disc_cv, 3), '\\tR2 =', np.round(r_disc_cv**2 * 100, 1),  '%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With Ridge and partial correlation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "age\n",
      "r = 0.52 \tp = 0.001 \tR2 = 26.7 %\n",
      "CogTotalComp_AgeAdj\n",
      "r = 0.5 \tp = 0.001 \tR2 = 25.0 %\n",
      "PMAT24_A_CR\n",
      "r = 0.28 \tp = 0.001 \tR2 = 8.1 %\n",
      "Flanker_AgeAdj\n",
      "r = 0.15 \tp = 0.001 \tR2 = 2.1 %\n",
      "CardSort_AgeAdj\n",
      "r = 0.24 \tp = 0.001 \tR2 = 5.8 %\n",
      "PicSeq_AgeAdj\n",
      "r = 0.17 \tp = 0.001 \tR2 = 2.8 %\n"
     ]
    }
   ],
   "source": [
    "model = Ridge()\n",
    "random_state = 42\n",
    "cv = KFold(10, shuffle=True, random_state=random_state)\n",
    "\n",
    "bar_data_ridge = []\n",
    "\n",
    "for target_var in ['age', 'CogTotalComp_AgeAdj', 'PMAT24_A_CR', 'Flanker_AgeAdj', 'CardSort_AgeAdj', 'PicSeq_AgeAdj']:\n",
    "    print(target_var)\n",
    "    X, y = create_data(target=target_var, feature_data=netmats_parcor)\n",
    "\n",
    "    predicted_discovery_cv = np.zeros_like(y)\n",
    "    cor_per_fold = np.zeros(cv.n_splits)\n",
    "    i = 0\n",
    "    for train, test in cv.split(X=X, y=y):\n",
    "        model.fit(X=X[train], y=y[train])\n",
    "        predicted_discovery_cv[test] = model.predict(X=X[test])\n",
    "        cor_per_fold[i] = np.corrcoef(y[test], predicted_discovery_cv[test])[0,1]\n",
    "        i += 1\n",
    "    # correlation between the cross-validated predictions and observations in the discovery sample\n",
    "    # this is the correct, unbiased estimate!\n",
    "    # calculated as mean test performance across all folds\n",
    "    r_disc_cv = np.mean(cor_per_fold)\n",
    "    # finalize model by training it on the full discovery sample (without cross-validation)\n",
    "    final_model = model.fit(X=X, y=y)\n",
    "    # obtain predictions with the final model on the discovery sample, note that this model actually overfits this sample.\n",
    "    # we do this only to demonstrate biased estimates\n",
    "    predicted_discovery_overfit = final_model.predict(X=X)\n",
    "    # here we obtain the biased effect size (r) estimates for demonstrational purposes\n",
    "    r_disc_overfit = np.corrcoef(predicted_discovery_overfit, y)[0, 1]\n",
    "\n",
    "    # below we calculate permutation-based p-values for all three effect size estimates (in-sample unbiased, in-sample biased, out-of-sample)\n",
    "    # (one sided tests, testing for positive correlation)\n",
    "    p_disc_cv = permutation_test(predicted_discovery_cv, y, method='approximate', num_rounds=1000, func=lambda x, y: np.corrcoef(x, y)[1][0],seed=random_state)\n",
    "    p_disc_overfit = permutation_test(predicted_discovery_overfit, y, method='approximate', num_rounds=1000, func=lambda x, y: np.corrcoef(x, y)[1][0],seed=random_state)\n",
    "\n",
    "    bar_data_ridge.append(r_disc_cv)\n",
    "\n",
    "    print('r =', np.round(r_disc_cv, 2), '\\tp =', np.round(p_disc_cv, 3), '\\tR2 =', np.round(r_disc_cv**2 * 100, 1),  '%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *See the notebook called 'plot_results_FC.ipynb' for figures.*"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
